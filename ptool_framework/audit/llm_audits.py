"""
LLM-generated audits for traces.

This module provides tools for generating audits using LLMs:
- AuditGenerator: Generate structured audits from example traces
- LLMAudit: Audit that uses LLM for evaluation

These tools support William's vision of using agent experiences to
automatically generate quality checks.

Example:
    >>> from ptool_framework.audit.llm_audits import AuditGenerator
    >>>
    >>> # Generate audits from good traces
    >>> generator = AuditGenerator(model="deepseek-v3-0324")
    >>> audit = generator.generate_from_traces(good_traces, task_description="BMI calculation")
    >>>
    >>> # Use the generated audit
    >>> runner.add_audit(audit)
"""

from __future__ import annotations

import json
import re
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional, Tuple, Union, TYPE_CHECKING

if TYPE_CHECKING:
    import pandas as pd
    from ..traces import WorkflowTrace
    from ..react import ReActTrajectory
    from ..trace_store import ExecutionTrace

from .base import (
    AuditResult,
    AuditReport,
    AuditViolation,
    BaseAudit,
    StructuredAudit,
)
from .dataframe_converter import TraceDataFrameConverter, convert_trace
from .structured_audit import AuditDSL
from .typicality.patterns import extract_pattern, PatternStats


@dataclass
class GeneratedRule:
    """A rule generated by LLM."""
    name: str
    description: str
    check_code: str  # Python code that returns bool
    error_message: str
    severity: str = "error"


@dataclass
class GeneratedAudit:
    """An audit generated by LLM."""
    name: str
    description: str
    rules: List[GeneratedRule] = field(default_factory=list)
    task_context: str = ""


class LLMAudit(BaseAudit):
    """
    Audit that uses an LLM to evaluate traces.

    Instead of fixed rules, this audit sends the trace to an LLM
    and asks it to identify issues.

    Attributes:
        name: Audit name
        prompt_template: Template for the evaluation prompt
        model: LLM model to use

    Example:
        >>> audit = LLMAudit(
        ...     name="llm_reviewer",
        ...     prompt_template="Review this trace for issues: {trace}",
        ...     model="deepseek-v3-0324"
        ... )
        >>> report = audit.run(df, metadata)
    """

    def __init__(
        self,
        name: str,
        prompt_template: str,
        model: str = "deepseek-v3-0324",
        description: str = "",
        llm_backend: Optional[Callable] = None,
    ):
        """
        Initialize LLM audit.

        Args:
            name: Audit name
            prompt_template: Template with {trace} placeholder
            model: LLM model to use
            description: Human-readable description
            llm_backend: Custom LLM backend (for testing)
        """
        super().__init__(name, description)
        self.prompt_template = prompt_template
        self.model = model
        self.llm_backend = llm_backend

    def run(self, df: "pd.DataFrame", trace_metadata: Dict[str, Any]) -> AuditReport:
        """
        Run LLM evaluation on the trace.

        Args:
            df: DataFrame representation of trace
            trace_metadata: Trace metadata

        Returns:
            AuditReport with LLM's evaluation
        """
        from ..llm_backend import call_llm

        # Format trace for LLM
        trace_str = self._format_trace_for_llm(df, trace_metadata)

        # Build prompt
        prompt = self.prompt_template.format(
            trace=trace_str,
            goal=trace_metadata.get("goal", "Unknown"),
            **trace_metadata,
        )

        # Call LLM
        if self.llm_backend:
            response = self.llm_backend(prompt, self.model)
        else:
            response = call_llm(prompt, self.model)

        # Parse response
        violations, passed = self._parse_llm_response(response)

        result = AuditResult.PASS if not violations else AuditResult.FAIL

        return AuditReport(
            trace_id=trace_metadata.get("trace_id", "unknown"),
            result=result,
            violations=violations,
            passed_checks=passed,
            metadata={
                "audit_type": "llm",
                "model": self.model,
                "response": response[:500],  # Truncate for storage
            },
        )

    def _format_trace_for_llm(
        self,
        df: "pd.DataFrame",
        metadata: Dict[str, Any],
    ) -> str:
        """Format trace DataFrame for LLM consumption."""
        lines = [f"Goal: {metadata.get('goal', 'Unknown')}\n"]
        lines.append("Steps:")

        for idx, row in df.iterrows():
            lines.append(f"  {row['step_idx'] + 1}. {row['fn_name']}")
            lines.append(f"     Input: {row.get('input', 'N/A')[:200]}")
            lines.append(f"     Output: {row.get('output', 'N/A')[:200]}")
            lines.append(f"     Status: {row.get('status', 'N/A')}")
            if row.get('error'):
                lines.append(f"     Error: {row['error']}")
            lines.append("")

        if metadata.get("final_answer"):
            lines.append(f"Final Answer: {metadata['final_answer']}")

        return "\n".join(lines)

    def _parse_llm_response(
        self,
        response: str,
    ) -> Tuple[List[AuditViolation], List[str]]:
        """Parse LLM response into violations and passed checks."""
        violations = []
        passed = []

        # Look for structured output patterns
        # Expected format: ISSUE: <description> or PASS: <check>

        issue_pattern = r'ISSUE:\s*(.+?)(?=ISSUE:|PASS:|$)'
        pass_pattern = r'PASS:\s*(.+?)(?=ISSUE:|PASS:|$)'

        for match in re.finditer(issue_pattern, response, re.IGNORECASE | re.DOTALL):
            issue_text = match.group(1).strip()
            if issue_text:
                violations.append(AuditViolation(
                    audit_name=self.name,
                    rule_name="llm_check",
                    message=issue_text[:500],
                    severity="warning",  # LLM issues are warnings by default
                ))

        for match in re.finditer(pass_pattern, response, re.IGNORECASE | re.DOTALL):
            pass_text = match.group(1).strip()
            if pass_text:
                passed.append(pass_text[:100])

        # If no structured output, try to infer from response
        if not violations and not passed:
            response_lower = response.lower()
            if any(word in response_lower for word in ["error", "issue", "problem", "incorrect", "wrong"]):
                violations.append(AuditViolation(
                    audit_name=self.name,
                    rule_name="llm_check",
                    message=response[:500],
                    severity="warning",
                ))
            else:
                passed.append("general_check")

        return violations, passed


class AuditGenerator:
    """
    Generate structured audits from example traces using LLM.

    This implements William's vision of using good traces to
    automatically generate quality checks.

    Example:
        >>> generator = AuditGenerator(model="deepseek-v3-0324")
        >>>
        >>> # Generate from good traces
        >>> audit = generator.generate_from_traces(
        ...     good_traces,
        ...     task_description="Calculate BMI from weight and height"
        ... )
        >>>
        >>> # Use the audit
        >>> runner.add_audit(audit)
    """

    def __init__(
        self,
        model: str = "deepseek-v3-0324",
        llm_backend: Optional[Callable] = None,
    ):
        """
        Initialize the generator.

        Args:
            model: LLM model for generation
            llm_backend: Custom LLM backend (for testing)
        """
        self.model = model
        self.llm_backend = llm_backend
        self.converter = TraceDataFrameConverter()

    def generate_from_traces(
        self,
        traces: List[Union["WorkflowTrace", "ReActTrajectory", "ExecutionTrace", List[Dict]]],
        task_description: str,
        audit_name: str = "generated_audit",
    ) -> StructuredAudit:
        """
        Generate a structured audit from example traces.

        Args:
            traces: Good example traces
            task_description: Description of the task these traces perform
            audit_name: Name for the generated audit

        Returns:
            StructuredAudit with generated rules

        Example:
            >>> audit = generator.generate_from_traces(
            ...     good_bmi_traces,
            ...     task_description="Calculate BMI from weight and height"
            ... )
        """
        # Extract patterns and analyze traces
        patterns = []
        all_steps = []

        for trace in traces:
            df, _ = self.converter.convert(trace)
            pattern = extract_pattern(df)
            patterns.append(pattern)
            all_steps.extend(df["fn_name"].tolist())

        # Get pattern statistics
        stats = PatternStats.from_patterns(patterns)

        # Generate audit using LLM
        generated = self._generate_audit_with_llm(
            task_description=task_description,
            patterns=patterns,
            stats=stats,
            audit_name=audit_name,
        )

        # Convert to StructuredAudit
        return self._build_structured_audit(generated)

    def _generate_audit_with_llm(
        self,
        task_description: str,
        patterns: List[List[str]],
        stats: PatternStats,
        audit_name: str,
    ) -> GeneratedAudit:
        """Use LLM to generate audit rules."""
        from ..llm_backend import call_llm

        # Build prompt
        prompt = self._build_generation_prompt(task_description, patterns, stats)

        # Call LLM
        if self.llm_backend:
            response = self.llm_backend(prompt, self.model)
        else:
            response = call_llm(prompt, self.model)

        # Parse response
        rules = self._parse_generated_rules(response)

        return GeneratedAudit(
            name=audit_name,
            description=f"Auto-generated audit for: {task_description}",
            rules=rules,
            task_context=task_description,
        )

    def _build_generation_prompt(
        self,
        task_description: str,
        patterns: List[List[str]],
        stats: PatternStats,
    ) -> str:
        """Build prompt for audit generation."""
        # Show example patterns
        pattern_examples = []
        for i, pattern in enumerate(patterns[:5]):
            pattern_str = " -> ".join(pattern)
            pattern_examples.append(f"  {i+1}. {pattern_str}")

        prompt = f"""You are an expert at analyzing reasoning traces and creating quality checks.

TASK DESCRIPTION:
{task_description}

EXAMPLE REASONING PATTERNS (from good traces):
{chr(10).join(pattern_examples)}

PATTERN STATISTICS:
- Total patterns analyzed: {stats.total_patterns}
- Vocabulary size: {stats.vocab_size}
- Average pattern length: {stats.avg_pattern_length:.1f}
- Most common steps: {stats.most_common_steps(5)}

Based on these good examples, generate audit rules to check that new traces follow similar patterns.

OUTPUT FORMAT:
Generate 3-5 rules in this format:

RULE: <rule_name>
DESCRIPTION: <what this rule checks>
CHECK: <condition to check, using these variables: fn_names (list of function names), step_count (number of steps)>
ERROR: <error message if check fails>
SEVERITY: error|warning

Example:
RULE: has_extraction
DESCRIPTION: Trace must include an extraction step
CHECK: any("extract" in fn for fn in fn_names)
ERROR: Trace is missing extraction step
SEVERITY: error

Generate rules now:"""

        return prompt

    def _parse_generated_rules(self, response: str) -> List[GeneratedRule]:
        """Parse LLM response into rules."""
        rules = []

        # Pattern to match rule blocks
        rule_pattern = r'RULE:\s*(\w+)\s*\nDESCRIPTION:\s*(.+?)\s*\nCHECK:\s*(.+?)\s*\nERROR:\s*(.+?)\s*\nSEVERITY:\s*(\w+)'

        for match in re.finditer(rule_pattern, response, re.IGNORECASE | re.DOTALL):
            name = match.group(1).strip()
            description = match.group(2).strip()
            check_code = match.group(3).strip()
            error_msg = match.group(4).strip()
            severity = match.group(5).strip().lower()

            if severity not in ("error", "warning", "info"):
                severity = "error"

            rules.append(GeneratedRule(
                name=name,
                description=description,
                check_code=check_code,
                error_message=error_msg,
                severity=severity,
            ))

        return rules

    def _build_structured_audit(self, generated: GeneratedAudit) -> StructuredAudit:
        """Convert GeneratedAudit to StructuredAudit."""
        audit = StructuredAudit(
            name=generated.name,
            description=generated.description,
        )

        for rule in generated.rules:
            check_fn = self._build_check_function(rule)
            audit.add_rule(rule.name, check_fn, rule.error_message)

        return audit

    def _build_check_function(
        self,
        rule: GeneratedRule,
    ) -> Callable[["pd.DataFrame", Dict], bool]:
        """Build a check function from generated rule."""
        check_code = rule.check_code

        def check_fn(df: "pd.DataFrame", metadata: Dict) -> bool:
            # Create context for eval
            fn_names = df["fn_name"].tolist()
            step_count = len(df)
            goal = metadata.get("goal", "")

            # Safe eval with limited context
            try:
                context = {
                    "fn_names": fn_names,
                    "step_count": step_count,
                    "goal": goal,
                    "any": any,
                    "all": all,
                    "len": len,
                    "in": lambda x, y: x in y,
                }
                return eval(check_code, {"__builtins__": {}}, context)
            except Exception:
                # If eval fails, consider it a pass (fail-safe)
                return True

        return check_fn

    def generate_llm_audit(
        self,
        task_description: str,
        audit_name: str = "llm_review",
    ) -> LLMAudit:
        """
        Generate an LLMAudit for a task.

        Args:
            task_description: Description of the task
            audit_name: Name for the audit

        Returns:
            LLMAudit configured for the task

        Example:
            >>> audit = generator.generate_llm_audit(
            ...     "Calculate BMI from weight and height"
            ... )
        """
        prompt_template = f"""Review this reasoning trace for a task: {task_description}

TRACE:
{{trace}}

Analyze the trace and identify any issues. Format your response as:
ISSUE: <description of problem>
or
PASS: <what looks correct>

Be specific about any errors in logic, missing steps, or incorrect calculations."""

        return LLMAudit(
            name=audit_name,
            prompt_template=prompt_template,
            model=self.model,
            description=f"LLM review for: {task_description}",
            llm_backend=self.llm_backend,
        )


def create_reasoning_audit(
    task_description: str,
    expected_steps: Optional[List[str]] = None,
    step_order: Optional[List[Tuple[str, str]]] = None,
    model: str = "deepseek-v3-0324",
) -> StructuredAudit:
    """
    Create a combined audit for reasoning tasks.

    Args:
        task_description: Description of the task
        expected_steps: Optional list of expected step names
        step_order: Optional order constraints
        model: LLM model for any LLM-based checks

    Returns:
        StructuredAudit with combined rules

    Example:
        >>> audit = create_reasoning_audit(
        ...     "Calculate BMI",
        ...     expected_steps=["extract_values", "calculate_bmi"],
        ...     step_order=[("extract_values", "calculate_bmi")]
        ... )
    """
    from .structured_audit import create_workflow_audit

    audit = create_workflow_audit(
        required_steps=expected_steps or [],
        step_order=step_order,
        name=f"reasoning_{task_description[:20].replace(' ', '_')}",
    )

    # Add basic quality checks
    audit.add_rule(
        "non_empty",
        lambda df, _: len(df) > 0,
        "Trace must have at least one step"
    )

    audit.add_rule(
        "no_failures",
        lambda df, _: (df["status"] != "failed").all() if len(df) > 0 else True,
        "No steps should have failed"
    )

    return audit
